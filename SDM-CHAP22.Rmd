---
title: "Chapter 22"
author: "Alan T. Arnholt - Modified notes from the Second Edition of _Probability and Statistics with R_"
date: 'Last compiled: `r format(Sys.time(), "%B %d, %Y at %X")`'
output: bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, warning = FALSE, message = FALSE, fig.align = "center")
library(tidyverse)
library(janitor)
```

# Comparing Counts

**Objectives:**

I.    Goodness of fit test

II.   Independence test

III.  Homogeneity test

IV.   What can go wrong?

________________


Many statistical procedures require knowledge of the population from which the sample is taken. For example,  using Student's $t$-distribution for testing a hypothesis or constructing a confidence interval for $\mu$ assumes that the parent population is normal. In this section, **goodness-of-fit** (GOF) procedures are presented that will help to identify the distribution of the population from which the sample is drawn. The null hypothesis in a goodness-of-fit test is a statement about the form of the cumulative distribution.  When all the parameters in the null hypothesis are specified, the hypothesis is called simple. Recall that in the event the null hypothesis does not completely specify all of the parameters of the distribution, the hypothesis is said to be composite. Goodness-of-fit tests are typically used when the form of the population is in question.  In contrast to most of the statistical procedures discussed so far, where the goal has been to reject the null hypothesis, in a GOF test one hopes to retain the null hypothesis. 

## The Chi-Square Goodness-of-Fit Test

Given a single random sample of size $n$ from an unknown population $F_X$, one may wish to test the hypothesis that $F_X$ has some known distribution $F_0(x)$ for all $x$.  For example, using the data frame `SOCCER` from the `PASWR2` package, is it reasonable to assume the number of goals scored during regulation time for the 232 soccer matches has a Poisson distribution with $\lambda=2.5$?

The chi-square goodness-of-fit test is based on a normalized statistic that examines the vertical deviations between what is observed and what is expected when $H_0$ is true in $k$ mutually exclusive categories. At times, such as in surveys of brand preferences, where the categories/groups would be the brand names, the sample will lend itself to being divided into $k$ mutually exclusive categories. Other times, the categories/groupings will be more arbitrary. Before applying the chi-square goodness-of-fit test, the data must be grouped according to some scheme to form $k$ mutually exclusive categories.  When the null hypothesis completely specifies the population, the probability that a random observation will fall into each of the chosen or fixed categories can be computed. Once the probabilities for a data point to fall into each of the chosen or fixed categories is computed, multiplying the probabilities by $n$ produces the expected counts for each category under the null distribution.  If the null hypothesis is true, the differences between the counts observed in the $k$ categories and the counts expected in the $k$ categories should be small.  The test criterion for testing $H_0: F_X(x) = F_0(x) \text{ for all } x$ against the alternative $H_1: F_X(x) \ne F_0(x)  \text{ for some } x$ when the null hypothesis is completely specified is

\begin{equation}
\chi_{\text{obs}}^2=\sum_{i=1}^{k} \frac{(O_k - E_k)^2}{E_k},
(\#eq:chiobs)
\end{equation}

where $\chi_\text{obs}^2$ is the sum of the squared deviations between what is observed $(O_k)$ and what is expected $(E_k)$ in each of the $k$ categories divided by what is expected in each of the $k$ categories.  Large values of $\chi_\text{obs}^2$ occur when the observed data are inconsistent with the null hypothesis and thus lead to rejection of the null hypothesis. The exact distribution of $\chi_\text{obs}^2$ is very complicated; however, for large $n$, provided all expected categories are at least 5, $\chi_\text{obs}^2$ is distributed approximately $\chi^2$ with $k-1$ degrees of freedom. When the null hypothesis is composite, that is, not all of the parameters are specified, the degrees of freedom for the random variable  $\chi_\text{obs}^2$ are reduced by one for each parameter that must be estimated.

____________

### Example{-}

Test the hypothesis that the number of goals scored during regulation time for the 232 soccer matches stored in the data frame `SOCCER` has a Poisson **cdf** with $\lambda=2.5$ with the chi-square goodness-of-fit test and an $\alpha$ level of 0.05. Produce a histogram showing the number of observed goals scored during regulation time and superimpose on the histogram the number of goals that are expected to be made when the distribution of goals follows a Poisson distribution with $\lambda=2.5$.

### Solution{-}

Since the number of categories for a Poisson distribution is
theoretically infinite, a table is first constructed of the observed
number of goals to get an idea of reasonable categories.

```{r}
library(PASWR2)
xtabs(~goals, data = SOCCER)
```

Based on the table, a decision is made to create categories for 0, 1, 2, 3, 4, 5, and 6 or more goals.  Under the null hypothesis that $F_0(x)$ is a Poisson distribution with $\lambda=2.5$, the probabilities of scoring 0, 1, 2, 3, 4, 5, and 6 or more goals are computed with `R` as follows:

```{r}
PX <- c(dpois(0:5, 2.5), ppois(5, 2.5, lower = FALSE))
PX
```

Since there were a total of $n=232$ soccer games, the expected number of goals for the six categories is simply $232 \times \tt{PX}$.

```{r}
EX <- 232*PX
OB <- c(as.vector(xtabs(~goals, data = SOCCER)[1:6]), 
        sum(xtabs(~goals, data = SOCCER)[7:9]))
OB
ans <- cbind(PX, EX, OB)
row.names(ans) <- c(" X=0"," X=1"," X=2"," X=3"," X=4"," X=5","X>=6")
ans
```

**Hypotheses**--- The null and alternative hypotheses for using the chi-square goodness-of-fit test to test the hypothesis that the number of goals scored during regulation time for the 232 soccer matches stored in the data frame `SOCCER` has a Poisson **cdf** with $\lambda=2.5$ are

\begin{align*}
        H_0&: F_X(x) = F_0(x) \sim \text{Pois}(\lambda=2.5)\text{ for all } x \text{ versus
        }\\
        H_A&:  F_X(x) \ne F_0(x) \text{ for some } x.
\end{align*}

**Test Statistic:**--- The test statistic chose is $\chi_{\text{obs}}^2.$

**Rejection Region Calculations**---Reject if $\chi^2_{\text{obs}}>\chi^2_{1-\alpha;k-1}$.  The $\chi^2_{\text{obs}}$ is computed with \@ref(eq:chiobs) in `R` below.

```{r}
chi_obs <- sum((OB - EX)^2/EX)
chi_obs
```

$`r chi_obs`= \chi^2_{\text{obs}}\overset{?}{>}\chi^2_{0.95;6}=`r qchisq(0.95, 6)`$.

**Statistical Conclusion**---The $p-$value is $`r pchisq(chi_obs, 6, lower = FALSE)`$.

```{r}
p_val <- pchisq(chi_obs, 7-1, lower = FALSE)
p_val
```
I.  Since $\chi^2_{\text{obs}}=`r chi_obs`$ is not greater than $\chi^2_{0.95;6}=`r qchisq(0.95, 6)`$, fail to reject $H_0$.

II. Since the $p-$value = $`r p_val`$ is greater than 0.05, fail to reject $H_0$.

Fail to reject $H_0$.

**English Conclusion**---There is no evidence to suggest that the true **cdf** does not equal the Poisson distribution with $\lambda=2.5$ for at least one $x$.

____________

To perform a goodness-of-fit test with the function `chisq.test()`, one may  specify a vector of observed values for the argument `x=`, and a vector of probabilities of the same length as the vector passed to `x=` to the argument `p=`.

```{r}
chisq.test(x = OB, p = PX)
```

The code below uses base graphics to create a  histogram with superimposed expected goals and the result is shown in Figure \@ref(fig:histo).

```{r, label = "histo", fig.cap = "Histogram of observed goals for `SOCCER` with a superimposed Poisson distribution with lambda = 2.5 (vertical lines)"}
hist(SOCCER$goals, breaks = c((-0.5 + 0):(8 + 0.5)), col = "lightblue", 
     xlab = "Goals scored", ylab = "", freq = TRUE, main = "")
x <- 0:8
fx <- (dpois(0:8, lambda = 2.5))*232
lines(x, fx, type = "h")
lines(x, fx, type = "p", pch = 16)
```

_________________

Although the chi-square goodness-of-fit test is primarily designed for discrete distributions, it can also be used with a continuous distribution if appropriate categories are defined.

____________

### Example{-}

Use the chi-square goodness-of-fit test with $\alpha=0.05$ to test the hypothesis that the SAT scores stored in the data frame `GRADES` have a normal **cdf**.  Use categories $(-\infty, \mu-2\sigma]$, $(\mu-2\sigma, \mu-\sigma]$, $(\mu-\sigma, \mu]$, $(\mu, \mu+\sigma]$,  $(\mu+\sigma, \mu+2\sigma]$, and $(\mu+2\sigma, \infty]$.  Produce a histogram using the categories specified and superimpose on the histogram the expected number of SAT scores in each category when $F_0(x)\sim N(\mu=\bar{x}, \sigma=s)$.

____________

### Solution{-}

**Hypotheses**---The null and alternative hypotheses for using the chi-square goodness-of-fit test to test the hypothesis that the SAT scores stored in the data frame `GRADES` have a Normal **cdf** are

\begin{align*}
        H_0&: F_X(x) = F_0(x) \sim N(\mu=\bar{x},\, \sigma=s)\text{ for all } x \text{ versus
        }\\
        H_A&:  F_X(x) \ne F_0(x)\text{ for some } x.
\end{align*}

**Test Statistic**---Since the mean and standard deviation are unknown, the first step is to estimate the unknown parameters $\mu$ and $\sigma$ using $\bar{x}=`r mean(GRADES$sat)`$ and  $s=`r sd(GRADES$sat)`$.
        
```{r}
mu <- mean(GRADES$sat)
sig <- sd(GRADES$sat) 
c(mu, sig)
```
        
Because a normal distribution is continuous, it is necessary to create categories that include all the data.  The categories $\mu - 3\sigma$  to $\mu -2\sigma, \bullets, \mu +2\sigma$  to $\mu + 3\sigma$ are `r mu - 3*sig` to `r mu - 2*sig`, `r mu - 2*sig` to  `r mu - 1*sig`, `r mu - 1*sig` to `r mu`, 
`r mu` to
`r mu + sig`, `r mu + sig` to `r mu + 2*sig`, and `r mu + 2*sig` to `r mu + 3*sig`.   These particular categories include all of the observed SAT scores;  however, the probabilities actually computed for the largest and smallest categories will be all of the area to the right and left, respectively, of $\bar{x} \pm 2s$.  This is done so that the total area under the distribution in the null hypothesis is one.

```{r}
bin <- seq(from = mu - 3*sig, to = mu + 3*sig, by = sig)
round(bin, 0)                     # vector of bin cut points
T1 <- table(cut(GRADES$sat, breaks = bin))
T1                                # count of observations in bins
OB <- as.vector(T1)
OB                                # vector of observations
PR <- c(pnorm(-2), pnorm(-1:2) - pnorm(-2:1), 
        pnorm(2, lower = FALSE))  # area under curve
EX <- 200*PR                      # Expected count in bins
ans <- cbind(PR, EX, OB)          # column bind values in ans
ans
```



**Rejection Region Calculations**---Reject if $\chi^2_{\text{obs}}>\chi^2_{1-\alpha;k-p-1}$.  Now that the expected and observed counts for each of the categories are computed, the $\chi_{\text{obs}}^2$ value can be computed according to \@ref(eq:chiobs) and is `r chisq.test(x = OB, p = PR)$stat`.

```{r}
chi_obs <- sum((OB - EX)^2/EX)
chi_obs
```

**Statistical Conclusion**---In this problem, two parameters were estimated, and as a consequence, the degrees of freedom are computed as $6-2-1=3$. The $p-$value is `r pchisq(chi_obs, 3, lower = FALSE)`.

```{r}
p_val <- pchisq(chi_obs, 6 - 2 - 1, lower = FALSE)
p_val
```

I.    Since $\chi^2_{\text{obs}}=`r chisq.test(x = OB, p = PR)$stat`$ is not greater than $\chi^2_{0.95;3}=`r qchisq(0.95, 3)`$, fail to reject $H_0$.

II.   Since the $p-$value = `r p_val` is greater than 0.05, fail to reject $H_0$.

**Fail to reject $H_0$**

**English Conclusion**---There is no evidence to suggest that the true **cdf** of SAT scores is not a normal distribution.

_____________

**Caution:**If one uses the `R` function `chisq.test()`, the degrees of
freedom and the subsequent $p-$value will be incorrect, as illustrated
below.

```{r}
chisq.test(x = OB, p = PR)  # returns incorrect dof and p-value
```

Since it is not feasible to produce a histogram that extends from $-\infty$ to $\infty$, a histogram is created where the categories will simply cover the range of observed values.\IE{hist}  In this problem, the range of the SAT scores is `r range(GRADES$sat)[1]` to `r range(GRADES$sat)[2]`.  The histogram with categories $(\mu-3\sigma, \mu-2\sigma]$, $(\mu-2\sigma, \mu-\sigma]$, $(\mu-\sigma, \mu]$, $(\mu+, \mu+\sigma]$,  $(\mu+\sigma, \mu+2\sigma]$, and $(\mu+2\sigma, \mu+3\sigma]$, superimposed with the expected number of SAT scores for the categories $(-\infty, \mu-2\sigma]$, $(\mu-2\sigma, \mu-\sigma]$, $(\mu-\sigma, \mu]$, $(\mu, \mu+\sigma]$, $(\mu+\sigma, \mu+2\sigma]$, and $(\mu+2\sigma, \infty]$ is computed below and depicted in Figure \@ref(fig:normsat).

```{r, label = "normsat", fig.cap = "Histogram of SAT scores in `Grades`"}
hist(GRADES$sat, breaks = bin, col = "lightblue", 
     xlab = "SAT scores", ylab="", freq = TRUE, main = "")
x <- bin[2:7] - sig/2
fx <- PR*200
lines(x, fx, type = "h")
lines(x, fx, type = "p", pch = 16)
```

____________

**Your Turn**

A psychology professor reports that historically grades in her intro class have been distributed
15% A, 30% B, 40% C, 10% D, and 5% F. Grades this year were distributed:

```{r, echo = FALSE}
OB <- c(89, 121, 78, 25, 12)
names(OB) <- LETTERS[1:5]
OB
```

Is there evidence that this year’s distribution is different from the historical distribution? If yes, which grade impacted that decision most?

```{r}
# Your Code Here
OB <- c(89, 121, 78, 25, 12)
names(OB) <- LETTERS[1:5]
OB
EX <- sum(OB)*c(0.15, 0.3, 0.4, 0.1, 0.05)
SS <- (OB - EX)^2/EX
SS
# Largest component is A's
chi_obs <- sum(SS)
p_val <- pchisq(chi_obs, 4, lower = FALSE)
p_val
chisq.test(x = OB, p = c(0.15, 0.3, 0.4, 0.1, 0.05))
```

____________

Given a contingency table (Question 5)

```{r}
ad <- c(20, 45, 35, 25, 50, 25)
adm <- matrix(ad, byrow = TRUE, nrow = 2)
adm
dimnames(adm) <- list(Branch = c("In-Town", "Mall"), Age = c("Less Than 30", "30-55", "56 or older"))
adm
chisq.test(adm)
# To get the expected counts use $exp
chisq.test(adm)$exp
```

To get **Standardized Residuals** compute $\frac{(\text{Obs - Exp})}{\sqrt{\text{Exp}}}$.  If using the `chisq.test()` function one may extract the residuals using `$residuals` 

```{r}
chisq.test(adm)$residuals
# Equivalent to
(chisq.test(adm)$obs - chisq.test(adm)$exp)/chisq.test(adm)$exp^.5
```


**Questions 8**

Given a ratio 9:3:3:1, the fraction for each category will be 9/16, 3/16, 3/16, 1/16. 

```{r}
obs <- c(54, 21, 13, 12)
obs2 <- obs*2
p <- c(9/16, 3/16, 3/16, 1/16)
chisq.test(x = obs, p = p)
chisq.test(obs2, p = p)
```

__________

## Independence test

**SCENARIO ONE:**  Is there an association between gender and a person's happiness? To investigate whether happiness depends on gender, one might use information collected from the General Social Survey (GSS) [http://sda.berkeley.edu/GSS](http://sda.berkeley.edu/GSS). In each survey, the GSS asks, ``Taken all together, how would you say things are these days --- would you say that you are very happy, pretty happy, or not too happy?'' Respondents to each survey are coded as either male or female. The information below shows how a subset of respondents (26-year-olds) were classified with respect to the variables HAPPY and SEX.

```{r}
HA <- c(110, 277, 50, 163, 302, 63)
HAT <- matrix(data = HA, nrow = 2, byrow = TRUE)
dimnames(HAT) <- list(SEX = c("Male", "Female"),
    Category = c("Very Happy", "Pretty Happy", "Not To Happy"))
HAT
```

Scenario one asks if there is an association between gender and a person's happiness.  Two events, $A$ and $B$, were defined as independent when $P(A \cap B)=P(A)\times P(B)$ or, equivalently, when $P(A|B)=P(A)$. If, instead of having a random sample from a single population, an $I \times J$ contingency table consisted of entries from the population, association could be mathematically verified by showing that $P(n_{ij}) \ne P(n_{i\bullet}) \times P(n_{\bullet j})$ for some $i$ and $j$. If by chance $P(n_{ij}) = P(n_{i\bullet}) \times P(n_{\bullet j})$  for all $i$ and $j$, then one would conclude there is no association between gender and a person's happiness.  That is, the variables gender and happiness would be considered mathematically independent. Since the entire population is not given but rather a sample from a population, the values in the $I \times J$ contingency table can be expected to change from sample to sample. The question is, ``By how much can the variables deviate from the mathematical definition of independence and still be considered statistically independent?''

The null and alternative hypotheses to test for independence between row and column variables is written $H_0: \pi_{ij}=\pi_{i\bullet}\pi_{\bullet j}$ versus $H_A:\pi_{ij} \ne \pi_{i\bullet}\pi_{\bullet j}$. The  test statistic is

\begin{equation}
\chi_{\text{obs}}^2 =\sum_{i=1}^I\sum_{j=1}^J \frac{(O_{ij} - E_{ij})^2}{E_{ij}}
(\#eq:ChiSqStatIndep)
\end{equation}

It compares the observed frequencies in the table with the expected frequencies when $H_0$ is true. Under the assumption of independence, and when the observations in the cells are sufficiently large (usually greater than 5), $\chi_{\text{obs}}^2 =\sum_{i=1}^{I}\sum_{j=1}^J \frac{(n_{ij}-\hat{\mu}_{ij})^2}{\hat{\mu}_{ij}} \overset{\bullet}{\sim} \chi^2_{(I-1)(J-1)},$ where $\hat{\mu}_{ij}=\frac{n_{i\bullet}n_{\bullet j}}{n} = E_{ij}$ and $n_{ij} = O_{ij}$.  The null hypothesis of independence is rejected when $\chi_{\text{obs}}^2 > \chi^2_{1-\alpha; (I-1)(J-1)}$.

The chi-squared approximation is generally satisfactory if the $E_{ij}$s ($\hat{\mu}_{ij}$s) in the test statistic are not too small.  Various rules of thumb exist for what might be considered too small.  A very conservative rule is to require all $E_{ij}$s to be 5 or more.  This can be accomplished by combining cells with small $E_{ij}$s and reducing the overall degrees of freedom.  At times, it may be permissible to let the $E_{ij}$ of a cell be as low as 0.5.

_____________

**Test For SCENARIO ONE:**

**Hypotheses** ---  $H_0: \pi_{ij} = \pi_{i\bullet }\pi_{\bullet j}$ 
        (Row and column variables are independent.)
        versus $H_A: \pi_{ij} \ne \pi_{i\bullet }\pi_{\bullet j}$ for at least one $i, j$
        (Row and column variables are dependent.)
        
**Test Statistic** --- The test statistic is
        $$\chi_{\text{obs}}^2=\sum_{i=1}^I \sum_{j=1}^J 
        \frac{(O_{ij}-E_{ij})^2}{E_{ij}} \overset{\bullet}{\sim} \chi^2_{(I-1)(J-1)}=
        \chi^2_{(2-1)(3-1)}=\chi^2_2$$
        under the assumption of independence. The $\chi^2_{\text{obs}}$ value is
        $`r chisq.test(HAT)$stat`$. 
        
```{r}
chisq.test(HAT)$stat
```

**Rejection Region Calculations** --- The rejection region is $$\chi_{\text{obs}}^2 > \chi^2_{1-\alpha; (I-1)(J-1)}=\chi^2_{0.95;2}= `r qchisq(0.95, 2)`.$$  Before the statistic $\chi^2_{\text{obs}}= \sum_{i=1}^I\sum_{j=1}^J \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$ can be computed, the expected counts for each of the $ij$ cells must be calculated.  Note that $O_{ij}=n_{ij}$ and $E_{ij}=\frac{n_{i\bullet}n_{\bullet j}}{n}$.

```{r}
E <- chisq.test(HAT)$exp
E
```

$$\chi^2_{\text{obs}}=\frac{(110 - 123.6280)^2}{123.6280} +
\frac{(277-262.2)^2}{262.2}+ \dots +\frac{(63-61.828)^2}{61.828}=4.3215.$$

The value of the test statistic is $\chi^2_{obs}= `r chisq.test(HAT)$stat`$.
This can be done with code by entering

```{r}
chisq.test(HAT)$stat
# Or
chi_obs <- sum((HAT - E)^2/E)
chi_obs
```

$`r chisq.test(HAT)$stat` = \chi^2_{\text{obs}} \overset{?}{>} \chi^2_{0.95,2}= `r qchisq(0.95, 2)`$.

**Statistical Conclusion** --- The $p-$value is `r chisq.test(HAT)$p.value`

```{r}
chisq.test(HAT)
chisq.test(HAT)$p.value
# Or
p_val <- pchisq(chi_obs, 2, lower = FALSE)
p_val
```

I.   From the rejection region, since $\chi_{\text{obs}}^2=`r chi_obs` < \chi^2_{0.95;2}=`r qchisq(0.95, 2)`$. fail to reject the null hypothesis of independence.
           
II.   Since the $p-$value = $`r p_val`$ is greater than 0.05, fail to reject the null hypothesis of independence.
       
**Fail to reject $H_0$.**

**English Conclusion** --- There is not sufficient evidence to suggest the variables gender and happiness are statistically dependent.

_______________

### EXAMPLE{-}

Is there an association between eye color and hearing loss in patients suffering from meningitis?  British researcher [Helen Cullingham]() recorded the eye color of 130 deaf patients and noted whether the deafness had followed treatment for meningitis.

```{r}
HC <- c(30, 72, 2, 26)
HCM <- matrix(HC, byrow = TRUE, nrow = 2)
dimnames(HCM) <- list(Eye_Color = c("Light", "Dark"), Deafness_related_to = c("Meningitis", "Other"))
HCM
```

```{r}
#Your code to test the appropriate hypothesis

chisq.test(HCM, correct = FALSE)
```

________________________ 

**SCENARIO TWO:**  In a double blind randomized drug trial (neither the patient nor the physician evaluating the patient knows the treatment, drug or placebo, the patient receives), 400 male patients with mild dementia were randomly divided into two groups of 200.  One group was given a placebo over three months while the second group received an experimental drug for three months.  At the end of the three months, the physicians (all  psychiatrists) classified the 400 patients into one of three categories: improved, no change,  or worse.   The information below shows how the psychiatrists classified the patients.  Are the proportions in the three status categories the same for the two treatments?

```{r}
DT <- c(67, 76, 57, 48, 73, 79)
DTT <- matrix(data = DT, nrow = 2, byrow = TRUE)
dimnames(DTT) <- list(Treatment = c("Drug", "Placebo"),
   Category = c("Improve", "No Change", "Worse"))
DTT
```


